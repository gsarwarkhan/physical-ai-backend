---
id: hardware-setup-personalization
title: 'Hardware Setup & Personalization'
sidebar_label: 'Hardware Setup'
---

# Hardware Setup & Personalization: Tailoring Your Physical AI Environment

Physical AI and humanoid robotics demand significant computational resources, especially for tasks involving real-time sensor processing, complex motor control, and advanced machine learning models. This chapter outlines various hardware setups you can use throughout this textbook, allowing you to personalize your learning experience based on your available resources.

## Why Physical AI Demands High VRAM

Many contemporary AI models, particularly those used in computer vision (e.g., for object detection, pose estimation) and large language models (LLMs) integrated into robotic decision-making, are extremely memory-intensive.

-   **Model Size**: State-of-the-art neural networks can have billions of parameters. Loading these models into memory requires substantial Video RAM (VRAM) on your GPU.
-   **High-Resolution Sensor Data**: Processing high-resolution camera feeds (RGB-D, stereo vision) or point clouds from LiDAR in real-time necessitates significant VRAM to store intermediate feature maps and processed data.
-   **Batch Processing**: Training or inference often involves processing data in batches. Larger batch sizes can improve throughput but demand more VRAM.
-   **Simulation**: Realistic physics simulations and rendering environments for training policies (e.g., in Isaac Sim) consume considerable VRAM.

Therefore, while a powerful CPU is important, a robust GPU with ample VRAM is often the most critical component for a smooth and efficient Physical AI development workflow.

## Your Personalized AI Lab

This textbook supports three primary hardware configurations, each with its own advantages and setup considerations.

### 1. The Pro Lab: NVIDIA RTX 4090 Workstation

:::info Ideal for:
Serious researchers, hobbyists with high-end hardware, and anyone aiming for top-tier performance in local development and complex simulations.
:::

For an uncompromised experience, an NVIDIA RTX 4090 (or similar high-end GPU) based workstation is the gold standard.

-   **Advantages**:
    *   **Unparalleled Performance**: Execute large models, complex simulations, and deep learning training locally at high speeds.
    *   **Large VRAM**: Typically 24GB of GDDR6X VRAM, capable of handling most advanced AI models and high-resolution data streams.
    *   **Local Control**: Full control over your environment, data, and privacy.
-   **Setup Considerations**:
    *   **Hardware Investment**: Requires significant initial investment.
    *   **Cooling & Power**: Needs a robust power supply and adequate cooling for sustained performance.
    *   **Software Stack**: Install NVIDIA drivers, CUDA Toolkit, cuDNN, and your preferred deep learning frameworks (e.g., PyTorch, TensorFlow) configured for GPU acceleration.

```bash title="Example Software Setup (Linux)"
# Install NVIDIA drivers (version specific, check NVIDIA website)
sudo apt update
sudo apt install nvidia-driver-535 # Example version

# Install CUDA Toolkit (download from NVIDIA website, follow instructions)
# e.g., for Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda-repo-ubuntu2204-12-2-local_12.2.0-535.54.03-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-2-local_12.2.0-535.54.03-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-2-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt update
sudo apt install cuda-toolkit-12-2

# Add CUDA to PATH (add to ~/.bashrc or ~/.zshrc)
echo 'export PATH=/usr/local/cuda-12.2/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# Install cuDNN (download from NVIDIA website, requires developer account)
# Extract and copy files to CUDA installation directory

# Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 2. The Mobile Lab: NVIDIA Jetson Orin Nano

:::tip Focus:
Embedded AI, robotics at the edge, and developing for low-power, high-performance scenarios.
:::

The Jetson Orin Nano provides a powerful platform for embedded AI applications, suitable for direct integration into smaller robots or for learning about on-device inference.

-   **Advantages**:
    *   **Power Efficiency**: Designed for low-power operation, making it ideal for mobile robots and battery-powered systems.
    *   **Compact Size**: Small form factor allows for integration into a wide range of hardware.
    *   **Integrated NVIDIA Stack**: Benefits from the NVIDIA AI ecosystem (CUDA, cuDNN, TensorRT) optimized for the Jetson platform.
-   **Setup Considerations**:
    *   **Resource Constraints**: While powerful for its size, it has less compute and VRAM than a desktop GPU. Model optimization (e.g., quantization, TensorRT) becomes crucial.
    *   **Cross-Compilation/Deployment**: Development often involves cross-compiling or deploying from a host PC.
    *   **JetPack SDK**: Utilize NVIDIA JetPack SDK for flashing the OS and installing necessary libraries.

```bash title="Jetson Orin Nano Setup (On Device)"
# Update and upgrade system
sudo apt update && sudo apt upgrade -y

# Install common development tools
sudo apt install -y build-essential cmake git curl wget

# Install pip for Python packages
sudo apt install -y python3-pip
pip3 install --upgrade pip

# Install PyTorch for Jetson (specific versions are provided by NVIDIA)
# Check NVIDIA Jetson PyTorch forum for the correct wheel
# Example:
# wget https://developer.download.nvidia.com/compute/redist/jp/v511/pytorch/torch-2.0.0-cp38-cp38-linux_aarch64.whl
# pip3 install torch-2.0.0-cp38-cp38-linux_aarch64.whl

# Install torchvision
# git clone https://github.com/pytorch/vision
# cd vision
# python3 setup.py install
```

### 3. The Cloud Lab: AWS/Google Cloud

:::warning Best for:
Large-scale training, pay-as-you-go flexibility, and access to specialized hardware (e.g., TPUs, multiple GPUs) without local investment.
:::

Cloud platforms offer scalable and flexible access to high-end computational resources, perfect for those without powerful local hardware or for tasks requiring massive compute.

-   **Advantages**:
    *   **Scalability**: Easily scale up or down compute resources as needed.
    *   **Cost-Effective**: Pay only for what you use, avoiding large upfront hardware costs.
    *   **Variety of Hardware**: Access to various GPU instances (NVIDIA A100, H100) and specialized AI accelerators (TPUs).
    *   **Managed Services**: Leverage managed services for machine learning (e.g., AWS SageMaker, Google AI Platform).
-   **Setup Considerations**:
    *   **Cost Management**: Monitor usage carefully to avoid unexpected bills.
    *   **Data Transfer**: Be mindful of data transfer costs and latency when moving large datasets.
    *   **Cloud-Specific APIs/SDKs**: Familiarity with cloud provider's SDKs and CLI tools is necessary.
    *   **Security**: Configure security groups, IAM roles, and storage permissions appropriately.

```bash title="Example Cloud Setup (AWS EC2 - Ubuntu Instance)"
# Connect to your EC2 instance via SSH
ssh -i /path/to/your-key.pem ubuntu@ec2-XXX-XXX-XXX-XXX.compute-1.amazonaws.com

# Update and install build tools
sudo apt update
sudo apt install -y build-essential cmake git curl wget

# Install NVIDIA drivers, CUDA, cuDNN (similar to Pro Lab, but often pre-installed or easier via cloud AMIs)
# Many cloud providers offer Deep Learning AMIs with everything pre-configured.

# Install Python and pip
sudo apt install -y python3-dev python3-pip
pip3 install --upgrade pip

# Install deep learning frameworks
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install tensorflow[and-cuda] # Or appropriate version
```

## Switching Between Setups in the Textbook

Throughout this textbook, exercises and projects will indicate the recommended hardware setup. When switching between environments:

1.  **Environment Variables**: Ensure all necessary environment variables (e.g., API keys, path configurations) are correctly set for the active environment.
2.  **Dependencies**: Install Python packages and system-level dependencies specific to your chosen lab. Use `pip install -r requirements.txt` where provided.
3.  **Data Sync**: If working on local projects that require large datasets, consider using cloud storage (S3, GCS) for easy synchronization between your local and cloud environments, or transfer directly to your Jetson device.
4.  **Configuration Files**: Some projects might have configuration files (`config.yaml`, `.env`) that need to be adjusted based on the hardware in use.

By understanding these different environments, you can effectively navigate the challenges of Physical AI development, leveraging the right tools for each task.
